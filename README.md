# PokÃ©mon GO Data Scraper

[![Hourly Scraper](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_hourly.yml/badge.svg)](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_hourly.yml)<br>
[![Daily Scraper](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_daily.yml/badge.svg)](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_daily.yml)<br>
[![Weekly Scraper](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_weekly.yml/badge.svg)](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_weekly.yml)<br>
[![Monthly Scraper](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_monthly.yml/badge.svg)](https://github.com/nchungdev/pogo-scraper/actions/workflows/scrape_monthly.yml)<br>
![Last Updated](https://img.shields.io/github/last-commit/nchungdev/pogo-scraper/data)

A fully automated scraping system designed to fetch, cache, and publish structured PokÃ©mon GO game data.  
This project provides high-quality datasets updated on *hourly, daily, weekly, and monthly* schedules using GitHub Actions.

---

## ğŸ“Œ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Scraped Data Sources](#scraped-data-sources)
- [Automation Schedules](#automation-schedules)
- [Repository Layout](#repository-layout)
- [Running Locally](#running-locally)
- [Data Output](#data-output)
- [Extending the System](#extending-the-system)
- [License](#license)

---

## ğŸ“– Overview

This repository hosts a modular, scalable scraper system for collecting structured PokÃ©mon GO data from multiple public sources such as:

- PokÃ©mon GO Hub
- LeekDuck
- RaidNow
- PokÃ©API

The scraper outputs standardized JSON files which are automatically committed to a dedicated **data** branch and can be used as free static JSON APIs for apps or research.

---

## âœ¨ Features

- **Automated multi-frequency scraping**
    - Hourly updates
    - Daily updates
    - Weekly updates
    - Monthly updates
- **HTML caching system**
    - Metadata timestamping
    - Cache expiry rules
    - GitHub Action HTML restoration
- **Fully modular scraper architecture**
    - Each scraper inherits from `BaseScraper`
    - Each group of scrapers has its own pipeline
- **Resilient**
    - Retries, delays, timeouts
    - Graceful fallback even when scraping partially fails
- **Data stored in a separate branch**
    - Allows clean separation between code and generated data
- **Extensible**
    - Add new scrapers or pipelines easily

---

## ğŸ¯ Scraped Data Sources

| Category | Source | Frequency |
|---------|--------|-----------|
| Type Chart | PokÃ©mon GO Hub | daily |
| PokÃ©mon Detail | PokÃ©mon GO Hub | weekly/monthly |
| Moves (PvE/PvP) | PokÃ©mon GO Hub | weekly |
| Raids / Eggs / Rocket | LeekDuck | hourly |
| RaidNow Feed | RaidNow | hourly |
| PokÃ©mon Species List | PokÃ©API | cached / as needed |

---

## â± Automation Schedules

Four workflows live in `.github/workflows/`:

| Workflow | Cron | Purpose |
|----------|------|---------|
| `scrape_hourly.yml` | Every hour | Raids, Eggs, Research, Rocket, RaidNow |
| `scrape_daily.yml` | Every 24h | Type Chart, Moves |
| `scrape_weekly.yml` | Weekly | PokÃ©mon Detail (partial) |
| `scrape_monthly.yml` | Monthly | Full PokÃ©mon Dataset rebuild |

All workflows push JSON + HTML cache to the `data` branch.

---

## ğŸ—‚ Repository Layout

```
project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”‚   â”œâ”€â”€ html_cache.py
â”‚   â”‚   â””â”€â”€ playwright_fetcher.py
â”‚   â”‚
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ utils.py
â”‚   â”‚   â”œâ”€â”€ normalize.py
â”‚   â”‚   â”œâ”€â”€ url_utils.py
â”‚   â”‚   â””â”€â”€ text_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ hourly_pipeline.py
â”‚   â”‚   â”œâ”€â”€ daily_pipeline.py
â”‚   â”‚   â”œâ”€â”€ weekly_pipeline.py
â”‚   â”‚   â””â”€â”€ monthly_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ pokemon/
â”‚   â”‚   â”œâ”€â”€ moves/
â”‚   â”‚   â”œâ”€â”€ raids/
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ events/
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ output/ (local only, gitignored)
â”‚   â”œâ”€â”€ html/
â”‚   â””â”€â”€ json/
â”‚
â”œâ”€â”€ data branch (GitHub only)
â”‚   â”œâ”€â”€ html/
â”‚   â””â”€â”€ json/
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§ª Running Locally

### 1. Clone & setup:

```sh
git clone https://github.com/nchungdev/pogo-scraper
cd pogo-scraper
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
playwright install --with-deps chromium
```

### 2. Run the entire pipeline:

```sh
python -m src.main
```

### 3. Run a specific mode:

```sh
python -m src.main --mode hourly
python -m src.main --mode daily
python -m src.main --mode weekly
python -m src.main --mode monthly
```

---

## ğŸ“¤ Data Output

All generated JSON & cached HTML are committed to the **data branch**, e.g.:

```
https://raw.githubusercontent.com/nchungdev/pogo-scraper/data/json/raid_bosses.json
https://raw.githubusercontent.com/nchungdev/pogo-scraper/data/json/type_chart.json
https://raw.githubusercontent.com/nchungdev/pogo-scraper/data/json/species_list.json
```

You can use them as free CDN-served JSON endpoints.

---

## â• Extending the System

To create a new scraper:

```sh
python tools/create_scraper.py category name --title "My Scraper"
```

The generator produces:

- Scraper class
- Parser folder
- Pipeline integration
- Imports in `__init__.py`

---

## ğŸ“„ License

Distributed under the MIT License.  
PokÃ©mon assets belong to The PokÃ©mon Company / Niantic â€“ this project is not affiliated with them.